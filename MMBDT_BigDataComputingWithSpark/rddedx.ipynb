{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive.  All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/\n",
    "For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set JAVA_HOME to: C:\\Users\\allen\\anaconda3\\Library\\bin\n",
      "Added to PATH: C:\\Users\\allen\\anaconda3\\Library\\bin\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Common Java installation paths on Windows\n",
    "possible_java_paths = [\n",
    "    r\"C:\\Program Files\\Java\\jdk-11.0.20\",\n",
    "    r\"C:\\Program Files\\Java\\jdk-17.0.8\", \n",
    "    r\"C:\\Program Files\\Java\\jdk1.8.0_381\",\n",
    "    r\"C:\\Program Files\\OpenJDK\\openjdk-11.0.2\",\n",
    "    r\"C:\\Users\\allen\\anaconda3\\Library\\bin\"  # Conda Java location\n",
    "]\n",
    "\n",
    "# Try to find Java\n",
    "java_home = None\n",
    "for path in possible_java_paths:\n",
    "    if os.path.exists(path):\n",
    "        java_home = path\n",
    "        break\n",
    "\n",
    "if java_home:\n",
    "    os.environ['JAVA_HOME'] = java_home\n",
    "    print(f\"Set JAVA_HOME to: {java_home}\")\n",
    "    \n",
    "    # Also add to PATH\n",
    "    java_bin = os.path.join(java_home, 'bin')\n",
    "    if java_bin not in os.environ['PATH']:\n",
    "        os.environ['PATH'] = java_bin + ';' + os.environ['PATH']\n",
    "        print(f\"Added to PATH: {java_bin}\")\n",
    "else:\n",
    "    print(\"Java not found in common locations. Please install Java.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Method 1: Using SparkSession (recommended for newer Spark versions)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Examples\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# OR Method 2: Direct SparkContext initialization (older approach)\n",
    "# conf = SparkConf().setAppName(\"RDD Examples\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from local file system:\n",
    "print(sc.version)\n",
    "\n",
    "fruits = sc.textFile('../data/fruits.txt')\n",
    "yellowThings = sc.textFile('../data/yellowthings.txt')\n",
    "print(fruits.collect())\n",
    "print(yellowThings.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from HDFS :\n",
    "fruits = sc.textFile('hdfs://url:9000/pathname/fruits.txt')\n",
    "fruits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "##  RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "fruitsReversed.cache()\n",
    "# try changing the file and re-execute with and without cache\n",
    "print(fruitsReversed.collect())\n",
    "# What happens when you uncomment the first line and run the whole program again with cache()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'grap', 'lemon']\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "k = 5\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= k)\n",
    "print(shortFruits.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'p', 'p', 'l', 'e', 'b', 'a', 'n', 'a', 'n', 'a', 'c', 'a', 'n', 'a', 'r', 'y', ' ', 'm', 'e', 'l', 'o', 'n', 'g', 'r', 'a', 'p', 'l', 'e', 'm', 'o', 'n', 'o', 'r', 'a', 'n', 'g', 'e', 'p', 'i', 'n', 'e', 'a', 'p', 'p', 'l', 'e', 's', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "print(characters.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "print(fruitsAndYellowThings.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pineapple', 'canary melon', 'lemon', 'banana']\n"
     ]
    }
   ],
   "source": [
    "# intersection\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "print(yellowFruits.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange', 'pineapple', 'canary melon', 'lemon', 'bee', 'banana', 'butter', 'gold', 'sunflower', 'apple', 'grap', 'strawberry']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "print(distinctFruitsAndYellowThings.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.0.0/programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry']\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "print(numFruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon']\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "print(first3Fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "3888000\n"
     ]
    }
   ],
   "source": [
    "print(fruits.map(lambda fruit: len(fruit)).sum())\n",
    "print(fruits.map(lambda fruit: len(fruit)).reduce(lambda x, y: x*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'}\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'l', 'b', 'c', 'r', 'y', 'g', 'i', 's', 'a', 'e', 'n', ' ', 'm', 'o', 't', 'w']\n"
     ]
    }
   ],
   "source": [
    "letterSet = fruits.flatMap(lambda fruit: list(fruit)).distinct().collect()\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "0\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "# Wrong: Don't do this!! - local partitioning issue\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "\n",
    "print(rdd.collect())\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(counter)\n",
    "print(rdd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "accum = sc.accumulator(0) # 0 is accumulator initial value\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x # spark accumulator addition\n",
    "\n",
    "a = rdd.foreach(g)\n",
    "\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    return x * x\n",
    "\n",
    "a = rdd.map(g) # map is not action but only transformation, function applied lazily\n",
    "print(accum.value)\n",
    "# print(a.reduce(lambda x, y: x+y))\n",
    "a.cache()\n",
    "\"\"\"\n",
    "if not cached, a will not persist, two count actions, map will be executed twice \n",
    "(3rd 45 will be 90)\n",
    "\"\"\"\n",
    "tmp = a.count()\n",
    "print(accum.value)\n",
    "print(rdd.reduce(lambda x, y: x+y))\n",
    "\n",
    "tmp = a.count()\n",
    "print(accum.value)\n",
    "print(rdd.reduce(lambda x, y: x+y))\n",
    "\n",
    "# use reduce() instead of accumulator is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Pi using Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:====================================================>   (93 + 7) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.137640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# From the official spark examples.\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 100\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_: int) -> float:\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
      "[[0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637], [0.2989089469610531, 0.9200284403681511, 0.8335399438536119, 0.6561941296559215, 0.8560726554133006, 0.276532385241941, 0.5285979714433593, 0.6157778141908172, 0.9270452938438591, 0.2681373010610637]]\n"
     ]
    }
   ],
   "source": [
    "# Example: glom\n",
    "import sys\n",
    "from random import random\n",
    "\n",
    "a = sc.parallelize(range(0,100),10)\n",
    "print(a.collect())\n",
    "print(a.glom().collect())\n",
    "print(a.map(lambda _: random()).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]\n",
      "[0, 1, 3, 6, 10, 5, 11, 18, 26, 35, 10, 21, 33, 46, 60, 15, 31, 48, 66, 85]\n",
      "530\n",
      "[0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88]\n"
     ]
    }
   ],
   "source": [
    "# Example: mapPartition and mapPartitionWithIndex\n",
    "a = sc.parallelize(range(0,20),4)\n",
    "print(a.glom().collect())\n",
    "\n",
    "def f(it):\n",
    "    s = 0\n",
    "    l = []\n",
    "    for i in it:\n",
    "        s += i\n",
    "#        if s % 2 == 0:\n",
    "        l.append(s)\n",
    "    return l\n",
    "\n",
    "print(a.mapPartitions(f).collect())\n",
    "\n",
    "def f(it):\n",
    "    s = 0\n",
    "    for i in it:\n",
    "        s += i\n",
    "        yield s\n",
    "\n",
    "print(a.mapPartitions(f).sum())\n",
    "\n",
    "def f(index, it):\n",
    "    s = index\n",
    "    for i in it:\n",
    "        s += i\n",
    "        yield s\n",
    "\n",
    "print(a.mapPartitionsWithIndex(f).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.1418484\n"
     ]
    }
   ],
   "source": [
    "# Correct version for computing Pi\n",
    "from random import random, seed\n",
    "from time import time\n",
    "\n",
    "partitions = 100\n",
    "n = 100000 * partitions\n",
    "\n",
    "s = time()\n",
    "\n",
    "def f(index, it):\n",
    "    seed(index + s)\n",
    "    for i in it:\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions).mapPartitionsWithIndex(f).sum()\n",
    "\n",
    "print(\"Pi is roughly\", 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure and Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD variables are references\n",
    "A = sc.parallelize(range(10))\n",
    "B = A.map(lambda x: x*2)\n",
    "A = B.map(lambda x: x+1)\n",
    "A.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# Linear-time selection\n",
    "\n",
    "data = [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
    "A = sc.parallelize(data,2)\n",
    "k = 4\n",
    "\n",
    "while True:\n",
    "    x = A.first()\n",
    "    A1 = A.filter(lambda z: z < x)\n",
    "    A2 = A.filter(lambda z: z > x)\n",
    "    mid = A1.count()\n",
    "    if mid == k:\n",
    "        print(x)\n",
    "        break\n",
    "    if k < mid:\n",
    "        A = A1\n",
    "    else:\n",
    "        A = A2\n",
    "        k = k - mid - 1\n",
    "    A.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(10))\n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x)\n",
    "#B.cache()\n",
    "print(B.count())\n",
    "x = 3\n",
    "print(B.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(10))\n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x)\n",
    "B.cache()\n",
    "print(B.take(10))\n",
    "#print(B.collect())\n",
    "\n",
    "x = 3\n",
    "print(B.take(10))\n",
    "#print(B.collect())\n",
    "# collect() doesn't always re-collect data - bad design!\n",
    "# Always use take() instead of collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 2), (12, 1), (4, 1), (10, 1), (5, 2), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "print(numFruitsByLength.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Big', 1), ('Course', 2), ('Description', 1), ('Information', 1), ('Lecture', 1), ('This', 1), ('across', 1), ('amount', 1), ('and', 3), ('as', 1), ('both', 1), ('centers.', 1), ('cloud', 1), ('commodity', 1), ('computing', 1), ('course', 1), ('data', 4), ('emerge', 1), ('enabling', 1), ('even', 1)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "lines = sc.textFile('../data/course.txt')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add)\n",
    "print(counts.sortByKey().take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Big', 1), ('Course', 2), ('Description', 1), ('Information', 1), ('Lecture', 1), ('This', 1), ('across', 1), ('amount', 1), ('and', 3), ('as', 1), ('both', 1), ('centers.', 1), ('cloud', 1), ('commodity', 1), ('computing', 1), ('course', 1), ('data', 4), ('emerge', 1), ('enabling', 1), ('even', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(counts.sortBy(lambda x: x[1], False).take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ('Apple', (134, 'OK'))), (1, ('Apple', (135, 'OK'))), (1, ('Apple', (45, 'OK'))), (2, ('Orange', (53, 'OK'))), (3, ('TV', (34, 'OK'))), (5, ('Computer', (162, 'Error')))]\n"
     ]
    }
   ],
   "source": [
    "# Join simple example\n",
    "\n",
    "products = sc.parallelize([(1, \"Apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "#trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print(products.join(trans).take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split()])\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt\n",
    "lines = sc.textFile('../data/kmeans_data.txt', 5)  \n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt\n",
    "# lines = sc.textFile('../data/kmeans_bigdata.txt', 5)  \n",
    "# lines is an RDD of strings\n",
    "K = 3\n",
    "convergeDist = 0.01  \n",
    "# terminate algorithm when the total distance from old center to new centers is less than this value\n",
    "\n",
    "data = lines.map(parseVector).cache() # data is an RDD of arrays\n",
    "\n",
    "kCenters = data.takeSample(False, K, 1)  # intial centers as a list of arrays\n",
    "tempDist = 1.0  # total distance from old centers to new centers\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(lambda p: (closestPoint(p, kCenters), (p, 1)))\n",
    "    # for each point in data, find its closest center\n",
    "    # closest is an RDD of tuples (index of closest center, (point, 1))\n",
    "        \n",
    "    pointStats = closest.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "    # pointStats is an RDD of tuples (index of center,\n",
    "    # (array of sums of coordinates, total number of points assigned))\n",
    "    \n",
    "    newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "    # compute the new centers\n",
    "    \n",
    "    tempDist = sum(np.sum((kCenters[i] - p) ** 2) for (i, p) in newCenters)\n",
    "    # compute the total disctance from old centers to new centers\n",
    "    \n",
    "    for (i, p) in newCenters:\n",
    "        kCenters[i] = p\n",
    "        \n",
    "print(\"Final centers: \", kCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "lines = sc.textFile(\"../data/pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"../data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0)\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks) \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls, rank))\n",
    "    \n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    ranks = contribs.reduceByKey(add).map(lambda t: (t[0], t[1] * 0.85 + 0.15))\n",
    "\n",
    "print(ranks.top(5, lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join vs. Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = sc.parallelize([(1, \"Apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print(trans.join(products).take(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = {1: \"Apple\", 2: \"Orange\", 3: \"TV\", 5: \"Computer\"}\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "broadcasted_products = sc.broadcast(products)\n",
    "\n",
    "results = trans.map(lambda x: (x[0], broadcasted_products.value[x[0]], x[1]))\n",
    "#  results = trans.map(lambda x: (x[0], products[x[0]], x[1]))\n",
    "print(results.take(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
