{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9a098e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "542e7288",
   "metadata": {},
   "source": [
    "# 4.2.0 Attention\n",
    "# Key and Query\n",
    "A A B A _ (B)\n",
    "Keys: embeddings of sequence\n",
    "Query: what we want to find\n",
    "\n",
    "Assume A = [1 0] B=[0 1] (one-hot encoding)\n",
    "call them ei\n",
    "\n",
    "Step 1: Compute score for each key given query\n",
    "\n",
    "[1 0] [1 0] [0 1] [1 0] -> keys\n",
    "A     A     B     A\n",
    "\n",
    "score si = ki^T * q (taking the ith key and dotting it with the query)\n",
    "set query q = [0 1] find Bs\n",
    "\n",
    "Step 2: softmax\n",
    "A A B A\n",
    "0 0 1 0 => softmax: 1/6 1/6 1/2 1/6 (assume e=3) alpha-i\n",
    "\n",
    "Step 3: Compute output as a weighted sum of the input\n",
    "result: sum(i=1 to n)alpha-1*ei = (1/6[1 0]+1/6[1 0]+1/2[0 1]+1/6[1 0])\n",
    "        = [1/2 1/2]\n",
    "        compare to average: [3/4 1/4]\n",
    "\n",
    "we can make attention more peaked by amplifying the embeddings\n",
    "ki = Wk*ei  \n",
    "Wk= diagonal matrix with 10s on the diagonal  Wk = 10 0 \n",
    "                                                    0 10\n",
    "[10,0] [10,0] [0,10] [10,0]\n",
    "0      0      1      0\n",
    "what will new attention values be with these keys?\n",
    "[1/(e^10+3) 1/(e^10+3) e^10/(e^10+3) 1/(e^10+3)]\n",
    "\n",
    "- Original dot product attention: si = ki^T q\n",
    "- Scaled dot product attention: si = ki^T Wq\n",
    "- Equivalent to having two weight matrices: si = (Wk ki)^T(Wq q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d56f8c",
   "metadata": {},
   "source": [
    "# 4.2.1 Self-attention\n",
    "self-attention: builds on the idea of attention. Every word in a sequence is both a key and a query simultaneously\n",
    "\n",
    "Q: seq len x d matrix (d = embedding dimension = 2 for these slides)\n",
    "K: seq len x d matrix\n",
    "Wq = 0 1\n",
    "     0 1 no matter what the value is, we're going to look for Bs\n",
    "Wk = 10 0\n",
    "     0  10 \"booster\" as before\n",
    "\n",
    "E = (1 0     Wq = 0 1       Wk = 10 0\n",
    "    1 0           0 1             0 10\n",
    "    0 1\n",
    "    1 0)\n",
    "\n",
    "Q = E Wq = (0 1\n",
    "            0 1\n",
    "            0 1\n",
    "            0 1)  -> every token looking for if any B around\n",
    "K = E Wk = (10 0\n",
    "            10 0\n",
    "            0 10\n",
    "            10 0)\n",
    "scores: S = Q K^T\n",
    "        Sij = qi dot kj\n",
    "        S = (0 0 10 0\n",
    "            0 0 10 0\n",
    "            0 0 10 0\n",
    "            0 0 10 0) - rows represent attention scores\n",
    "e.g. row 1: \"I care about row 3\"\n",
    "-> row-wise softmax (A) => distribution per row\n",
    "\n",
    "output = AE (actually A(E W^v))\n",
    "\n",
    "Attention(Q,K,V) = softmax(Q * K^T / sqrt(dk)) V = Z\n",
    "Q = EWq, K=EWk, V=EWv\n",
    "where dk = dimension of the keys / vector dimension - control the scale, less peaked\n",
    "this is just one head of self-attention - produce multiple heads via randomly initialize parameter matrices\n",
    "\n",
    "What does self-attention produce?\n",
    "- Square attention matrix * input = same dimension as the input\n",
    "- Computes a contextualized encoding for each word, preserving the length of the sequence\n",
    "\n",
    "# Properties of self-attention\n",
    "complexity per layer: O(n^2 * d)\n",
    "sequential operation: O(1)\n",
    "max path length: O(1)\n",
    "n = sentence length, d = hidden dim, k = kernel size, r = restricted neighborhood size\n",
    "- Quadratic complexity, but O(1) sequential operations (not linear like in RNNs) and\n",
    "    O(1) \"path\" for words to inform each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4aa9a",
   "metadata": {},
   "source": [
    "![Image](4.2.1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce596940",
   "metadata": {},
   "source": [
    "![](4.2.1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f9c89",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "What is the complexity of computing the self-attention matrix for a sentence with \n",
    " words? Assume the input embeddings are size k_1, the key and value embeddings are size k_2, and we're doing naive matrix multiplication.\n",
    "\n",
    "1. Linear projection - X*W_q/h/v -> (n*k_1)*(k_1*k_2) -> O(n*k_1*k_2)\n",
    "2. Attention score - QK^T -> (n*k_2)*(k_2*n) -> O(n^2 * k_2)\n",
    "3. Softmax - Just elementwise operations on an (𝑛×𝑛) matrix → O(n ^2)\n",
    "4. Weighted sum - Multiply (n×n) attention scores with V(n×k_2) -> O(n^2*k_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcd75e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
