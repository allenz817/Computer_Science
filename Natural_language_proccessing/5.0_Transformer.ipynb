{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17419a93",
   "metadata": {},
   "source": [
    "# 5.0.0 Transformer Architecture\n",
    "- Alternate multi-head self-attention with feedforward layers that operate over each word individually\n",
    "- FFN(x) = max(0, xW_1+b_1)W_2 + b_2\n",
    "- These feedforward layers are where most of the parameters are\n",
    "- Residual connections in the model: input of a layer is added to its output\n",
    "- Layer normalization: controls the scale of different layers in very deep networks\n",
    "\n",
    "- Vectors: d_model (input / output / native dimension)\n",
    "- Queries/keys: d_k, always smaller than d_model\n",
    "- Values: separate dimension d_v, output is multiplied by Wo which is d_v x d_model so we can get back to d_model before the residual\n",
    "- FFN can explode the dimension with W_1 and collapse it back with W_2\n",
    "- FFN(x) = max(0, xW_1+b_1) W_2 + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20197e03",
   "metadata": {},
   "source": [
    "![](5.0.0_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5561e",
   "metadata": {},
   "source": [
    "![](5.0.0_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd7c53",
   "metadata": {},
   "source": [
    "# 5.0.1 Using Transformers\n",
    "What do Transformers Produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a74ba",
   "metadata": {},
   "source": [
    "![](5.0.1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8e247",
   "metadata": {},
   "source": [
    "![](5.0.1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c665d",
   "metadata": {},
   "source": [
    "![](5.0.1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bf265",
   "metadata": {},
   "source": [
    "![](5.0.1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103b404",
   "metadata": {},
   "source": [
    "# 5.0.2 Transformer Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36834fa6",
   "metadata": {},
   "source": [
    "![](5.0.2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83255111",
   "metadata": {},
   "source": [
    "![](5.0.2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16925f5",
   "metadata": {},
   "source": [
    "![](5.0.2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409986d",
   "metadata": {},
   "source": [
    "![](5.0.2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ab2ee",
   "metadata": {},
   "source": [
    "- Multiple sequences and multiple timesteps per sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38fe58",
   "metadata": {},
   "source": [
    "- Problem: model is \"cheating\" by looking at the next word, thus achieving perfect accuracy\n",
    "- Attention masking: casual mask (only look at the \"past\")\n",
    "- Implementation in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fcef2",
   "metadata": {},
   "source": [
    "![](5.0.2_5.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
