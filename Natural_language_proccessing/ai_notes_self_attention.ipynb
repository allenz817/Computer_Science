{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e356b",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism Implementation\n",
    "This notebook implements the self-attention mechanism with step-by-step calculations and visualizations based on class notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f24d71",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import NumPy, Matplotlib, and other necessary libraries for matrix operations and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605553f",
   "metadata": {},
   "source": [
    "## 2. Define Input Embeddings\n",
    "Create the embedding matrix E with one-hot encodings for tokens A and B as shown in the class notes example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence: A A B A (using one-hot encodings)\n",
    "# A = [1, 0], B = [0, 1]\n",
    "E = np.array([\n",
    "    [1, 0],  # A\n",
    "    [1, 0],  # A  \n",
    "    [0, 1],  # B\n",
    "    [1, 0]   # A\n",
    "])\n",
    "\n",
    "print(\"Input Embedding Matrix E:\")\n",
    "print(E)\n",
    "print(f\"Shape: {E.shape}\")\n",
    "print(\"\\nSequence representation: A A B A\")\n",
    "print(\"A = [1, 0], B = [0, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dab3d",
   "metadata": {},
   "source": [
    "## 3. Initialize Weight Matrices\n",
    "Define the weight matrices Wq, Wk, and Wv for query, key, and value transformations using the values from class notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight matrices from class notes\n",
    "Wq = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "Wk = np.array([\n",
    "    [10, 0],\n",
    "    [0, 10]\n",
    "])\n",
    "\n",
    "# For value matrix, we'll use the identity matrix initially\n",
    "Wv = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "print(\"Query Weight Matrix Wq:\")\n",
    "print(Wq)\n",
    "print(\"Interpretation: No matter what the value is, we're looking for Bs\")\n",
    "\n",
    "print(\"\\nKey Weight Matrix Wk:\")\n",
    "print(Wk)\n",
    "print(\"Interpretation: 'Booster' matrix - amplifies the embeddings\")\n",
    "\n",
    "print(\"\\nValue Weight Matrix Wv:\")\n",
    "print(Wv)\n",
    "print(\"Interpretation: Identity matrix for this example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa61be",
   "metadata": {},
   "source": [
    "## 4. Compute Query and Key Matrices\n",
    "Calculate Q = E * Wq and K = E * Wk matrices and display the results with explanations of what each represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Query matrix\n",
    "Q = E @ Wq\n",
    "print(\"Query Matrix Q = E @ Wq:\")\n",
    "print(Q)\n",
    "print(\"Interpretation: Every token is looking for Bs around\")\n",
    "\n",
    "# Compute Key matrix  \n",
    "K = E @ Wk\n",
    "print(\"\\nKey Matrix K = E @ Wk:\")\n",
    "print(K)\n",
    "print(\"Interpretation: Amplified embeddings for better distinction\")\n",
    "\n",
    "# Compute Value matrix\n",
    "V = E @ Wv\n",
    "print(\"\\nValue Matrix V = E @ Wv:\")\n",
    "print(V)\n",
    "print(\"Interpretation: Original embeddings (identity transformation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0f453",
   "metadata": {},
   "source": [
    "## 5. Calculate Attention Scores\n",
    "Compute the score matrix S = Q * K^T and explain how each element represents attention between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fef2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores\n",
    "S = Q @ K.T\n",
    "print(\"Score Matrix S = Q @ K^T:\")\n",
    "print(S)\n",
    "\n",
    "print(\"\\nInterpretation of scores:\")\n",
    "print(\"- Rows represent queries (each token asking)\")\n",
    "print(\"- Columns represent keys (each token being asked about)\")\n",
    "print(\"- High scores indicate strong attention\")\n",
    "\n",
    "# Explain specific scores\n",
    "print(f\"\\nExample: S[0,2] = {S[0,2]} - Token 1 (A) attending to Token 3 (B)\")\n",
    "print(f\"All tokens have high attention to Token 3 (B) because queries look for Bs\")\n",
    "\n",
    "# Display with labels\n",
    "score_df = pd.DataFrame(S, \n",
    "                       index=['A₁', 'A₂', 'B₃', 'A₄'], \n",
    "                       columns=['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "print(\"\\nScore Matrix with labels:\")\n",
    "print(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d22c23",
   "metadata": {},
   "source": [
    "## 6. Apply Softmax to Get Attention Weights\n",
    "Apply row-wise softmax to the score matrix to get the attention distribution A, including the scaling factor sqrt(dk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc169ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax function\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for array x along specified axis.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "# Apply scaling factor sqrt(dk)\n",
    "dk = K.shape[1]  # dimension of keys\n",
    "print(f\"Key dimension dk = {dk}\")\n",
    "print(f\"Scaling factor sqrt(dk) = {sqrt(dk)}\")\n",
    "\n",
    "# Scaled scores\n",
    "S_scaled = S / sqrt(dk)\n",
    "print(f\"\\nScaled Score Matrix S / sqrt(dk):\")\n",
    "print(S_scaled)\n",
    "\n",
    "# Apply row-wise softmax to get attention weights\n",
    "A = softmax(S_scaled, axis=1)\n",
    "print(f\"\\nAttention Weight Matrix A (after softmax):\")\n",
    "print(A)\n",
    "\n",
    "# Display with labels\n",
    "attention_df = pd.DataFrame(A, \n",
    "                           index=['A₁', 'A₂', 'B₃', 'A₄'], \n",
    "                           columns=['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "print(\"\\nAttention weights with labels:\")\n",
    "print(attention_df)\n",
    "\n",
    "# Verify that rows sum to 1\n",
    "print(f\"\\nRow sums (should be 1.0): {A.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0bd19",
   "metadata": {},
   "source": [
    "## 7. Compute Self-Attention Output\n",
    "Calculate the final output using the formula Attention(Q,K,V) = softmax(Q * K^T / sqrt(dk)) * V and show how it produces contextualized encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute self-attention output\n",
    "Z = A @ V\n",
    "print(\"Self-Attention Output Z = A @ V:\")\n",
    "print(Z)\n",
    "\n",
    "print(f\"\\nOutput shape: {Z.shape}\")\n",
    "print(\"Same as input shape - preserves sequence length!\")\n",
    "\n",
    "# Compare with original embeddings\n",
    "print(\"\\nComparison with original embeddings:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Token': ['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "    'Original_dim1': E[:, 0],\n",
    "    'Original_dim2': E[:, 1], \n",
    "    'SelfAttn_dim1': Z[:, 0],\n",
    "    'SelfAttn_dim2': Z[:, 1]\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Original A tokens: [1, 0]\")\n",
    "print(\"- Original B token: [0, 1]\") \n",
    "print(\"- After self-attention: All tokens are influenced by the B token\")\n",
    "print(\"- Each token now has a contextualized representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a95eb",
   "metadata": {},
   "source": [
    "## 8. Visualize Attention Matrix\n",
    "Create heatmaps and visualizations of the attention weights to show which tokens attend to which other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e846f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention heatmap\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Attention weights heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(A, annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "            yticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Self-Attention Weights Matrix')\n",
    "plt.xlabel('Keys (being attended to)')\n",
    "plt.ylabel('Queries (attending from)')\n",
    "\n",
    "# Plot 2: Score matrix heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(S, annot=True, fmt='.1f', cmap='Reds',\n",
    "            xticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "            yticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "            cbar_kws={'label': 'Raw Score'})\n",
    "plt.title('Raw Attention Scores Matrix')\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Attention pattern analysis\n",
    "print(\"Attention Pattern Analysis:\")\n",
    "print(\"- All tokens pay high attention to B₃ (column 3)\")\n",
    "print(\"- This is because all queries are looking for B tokens\")\n",
    "print(\"- The attention pattern shows the model focuses on relevant information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformation process\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original embeddings\n",
    "im1 = axes[0,0].imshow(E.T, cmap='RdYlBu', aspect='auto')\n",
    "axes[0,0].set_title('Original Embeddings E')\n",
    "axes[0,0].set_xlabel('Token Position')\n",
    "axes[0,0].set_ylabel('Embedding Dimension')\n",
    "axes[0,0].set_xticks(range(4))\n",
    "axes[0,0].set_xticklabels(['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "plt.colorbar(im1, ax=axes[0,0])\n",
    "\n",
    "# Attention weights\n",
    "im2 = axes[0,1].imshow(A, cmap='Blues', aspect='auto')\n",
    "axes[0,1].set_title('Attention Weights A')\n",
    "axes[0,1].set_xlabel('Keys')\n",
    "axes[0,1].set_ylabel('Queries')\n",
    "axes[0,1].set_xticks(range(4))\n",
    "axes[0,1].set_yticks(range(4))\n",
    "axes[0,1].set_xticklabels(['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "axes[0,1].set_yticklabels(['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "plt.colorbar(im2, ax=axes[0,1])\n",
    "\n",
    "# Self-attention output\n",
    "im3 = axes[1,0].imshow(Z.T, cmap='RdYlBu', aspect='auto')\n",
    "axes[1,0].set_title('Self-Attention Output Z')\n",
    "axes[1,0].set_xlabel('Token Position')\n",
    "axes[1,0].set_ylabel('Embedding Dimension')\n",
    "axes[1,0].set_xticks(range(4))\n",
    "axes[1,0].set_xticklabels(['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "plt.colorbar(im3, ax=axes[1,0])\n",
    "\n",
    "# Comparison plot\n",
    "x_pos = np.arange(4)\n",
    "width = 0.35\n",
    "axes[1,1].bar(x_pos - width/2, E[:, 0], width, label='Original dim1', alpha=0.7)\n",
    "axes[1,1].bar(x_pos + width/2, Z[:, 0], width, label='Self-attn dim1', alpha=0.7)\n",
    "axes[1,1].set_title('Dimension 1: Original vs Self-Attention')\n",
    "axes[1,1].set_xlabel('Token Position')\n",
    "axes[1,1].set_ylabel('Value')\n",
    "axes[1,1].set_xticks(x_pos)\n",
    "axes[1,1].set_xticklabels(['A₁', 'A₂', 'B₃', 'A₄'])\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34ab5d",
   "metadata": {},
   "source": [
    "## 9. Multi-Head Self-Attention Implementation\n",
    "Extend the single-head implementation to multiple heads using different randomly initialized parameter matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(E, num_heads=2, d_model=2):\n",
    "    \"\"\"\n",
    "    Implement multi-head self-attention\n",
    "    \n",
    "    Args:\n",
    "        E: Input embeddings (seq_len, d_model)\n",
    "        num_heads: Number of attention heads\n",
    "        d_model: Model dimension\n",
    "    \n",
    "    Returns:\n",
    "        output: Multi-head attention output\n",
    "        attention_weights: List of attention weight matrices for each head\n",
    "    \"\"\"\n",
    "    seq_len = E.shape[0]\n",
    "    d_k = d_model // num_heads  # dimension per head\n",
    "    \n",
    "    # Initialize random weight matrices for each head\n",
    "    heads_output = []\n",
    "    attention_weights = []\n",
    "    \n",
    "    print(f\"Multi-Head Self-Attention with {num_heads} heads\")\n",
    "    print(f\"d_model = {d_model}, d_k = {d_k}\")\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        print(f\"\\n--- Head {head + 1} ---\")\n",
    "        \n",
    "        # Random weight matrices for this head\n",
    "        Wq_h = np.random.randn(d_model, d_k) * 0.5\n",
    "        Wk_h = np.random.randn(d_model, d_k) * 0.5  \n",
    "        Wv_h = np.random.randn(d_model, d_k) * 0.5\n",
    "        \n",
    "        # Compute Q, K, V for this head\n",
    "        Q_h = E @ Wq_h\n",
    "        K_h = E @ Wk_h\n",
    "        V_h = E @ Wv_h\n",
    "        \n",
    "        # Compute attention scores and weights\n",
    "        S_h = Q_h @ K_h.T\n",
    "        S_h_scaled = S_h / sqrt(d_k)\n",
    "        A_h = softmax(S_h_scaled, axis=1)\n",
    "        \n",
    "        # Compute output for this head\n",
    "        Z_h = A_h @ V_h\n",
    "        \n",
    "        heads_output.append(Z_h)\n",
    "        attention_weights.append(A_h)\n",
    "        \n",
    "        print(f\"Head {head + 1} attention weights:\")\n",
    "        print(A_h.round(3))\n",
    "    \n",
    "    # Concatenate all heads and apply output projection\n",
    "    concatenated = np.concatenate(heads_output, axis=1)\n",
    "    \n",
    "    # Output projection (random matrix for demo)\n",
    "    W_o = np.random.randn(num_heads * d_k, d_model) * 0.5\n",
    "    output = concatenated @ W_o\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Apply multi-head attention\n",
    "multi_head_output, multi_attention_weights = multi_head_attention(E, num_heads=2)\n",
    "\n",
    "print(f\"\\nMulti-Head Attention Output:\")\n",
    "print(multi_head_output)\n",
    "print(f\"Shape: {multi_head_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68814f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-head attention patterns\n",
    "fig, axes = plt.subplots(1, len(multi_attention_weights), figsize=(15, 5))\n",
    "\n",
    "for i, attn_weights in enumerate(multi_attention_weights):\n",
    "    sns.heatmap(attn_weights, annot=True, fmt='.3f', cmap='viridis',\n",
    "                xticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "                yticklabels=['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "                ax=axes[i], cbar_kws={'label': 'Attention Weight'})\n",
    "    axes[i].set_title(f'Head {i+1} Attention Weights')\n",
    "    axes[i].set_xlabel('Keys')\n",
    "    axes[i].set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare single-head vs multi-head outputs\n",
    "print(\"Comparison: Single-Head vs Multi-Head Self-Attention\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Token': ['A₁', 'A₂', 'B₃', 'A₄'],\n",
    "    'Original_1': E[:, 0],\n",
    "    'Original_2': E[:, 1],\n",
    "    'SingleHead_1': Z[:, 0], \n",
    "    'SingleHead_2': Z[:, 1],\n",
    "    'MultiHead_1': multi_head_output[:, 0],\n",
    "    'MultiHead_2': multi_head_output[:, 1]\n",
    "})\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Each head learns different attention patterns\")\n",
    "print(\"2. Multi-head attention captures diverse relationships\")\n",
    "print(\"3. Different heads may focus on different aspects of the input\")\n",
    "print(\"4. The final output combines information from all heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8958c40",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete self-attention mechanism:\n",
    "\n",
    "1. **Basic Attention**: Showed how queries attend to keys to find relevant information\n",
    "2. **Self-Attention**: Extended to where each token is both query and key\n",
    "3. **Mathematical Framework**: Implemented the full formula: Attention(Q,K,V) = softmax(QK^T/√dk)V\n",
    "4. **Visualization**: Created heatmaps to understand attention patterns\n",
    "5. **Multi-Head Extension**: Showed how multiple attention heads capture different relationships\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Self-attention produces contextualized representations of the same length as input\n",
    "- The attention matrix shows which tokens influence each other\n",
    "- Scaling by √dk prevents extreme softmax values\n",
    "- Multiple heads allow the model to attend to different types of relationships simultaneously\n",
    "- This mechanism is fundamental to Transformer architectures"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
